# Word Embedding

### lecture-2

这一节主要是讲了Glove算法和如何评估词嵌入技术（metric）。

#### Glove

相比其他两种技术1. 共现矩阵分解（最大的一个问题是词频并不能完全展示词相似度，例如：a、the这种对词义影响很小但是频率很大的词却会对词向量的相似度计算产生很大影响，这是不合理的）2. 基于窗口的词向量学习（这种的一个问题就是没有使用到共现矩阵的统计信息，没有全局信息加持）。

共现矩阵$\text X$，即确定窗口下给定一个词，另一个词出现在这个词上下文的次数，$\text X_{ij}$，即第$j$个词在第$i$个词上下文出现的次数。

Glove和其他的方法不同的一点就是，Glove使用的是概率ratio而不是概率。在word2vec中，优化的目标其实是要使得$p(w_o|w_c)$最大，也就是给定词$w_c$情况下，上下文词为$w_o$的概率要最大。而Glove给出的是：
$$
r=\frac{P_{ik}}{P_{jk}}，P_{ik}=\frac{X_{ik}}{X_i}，就是k词在i当前词上下文中出现的概率
$$
如果$k$这个词是$i$这个词的上下文，而不是$j$这个词的上下文，那$r$应该是很大，反之很小，而且如果都无关，那就更倾向于于1。

$r$值是依赖$i,j,k$三个值，其实是依赖三个词，设有函数$F$（下面就是靠这种玄学（脑洞）来构造一个$F$）：
$$
F(w_i, w_j, \hat w_k) = \frac{P_{ik}}{P_{jk}}，考虑向量线性相关性，而向量减法很容易让人想到，所以有如下公式（玄学1） \\ 
F((w_i - w_j), \hat w_k) = \frac{P_{ik}}{P_{jk}}，考虑到现在w_i-w_j还是向量，而我们求得的是常量，则有如下转换（玄学2）\\
F((w_i - w_j)^T \hat w_k) = \frac{P_{ik}}{P_{jk}}，对比等式左右两边，左边可以扩展开如下所示\\
F((w_i - w_j)^T \hat w_k) = F(w_i^t\hat w_k - w_j^T\hat w_k)，左边是是一个加法的运算，而对应上式是一个乘法运算，\\我们要找到一个同态的F函数来做到这一点，就如下所示（玄学3）\\
F((w_i - w_j)^T \hat w_k) = \frac{F(w_i^T\hat w_k)}{F(w_j^T\hat w_k)}，同态就是说，在左右等式的集合内，加法运算的结果和乘法运算的结果可以一一\\对应（或群组对应一个）\\
P_{ik} = F(w_i^T\hat w_k)，F=\exp，按照上面的说法，F只能是\exp函数，则带入到分子中（分母也一样），有：\\
w_i^T\hat w_k = \log P_{ik} = \log\frac{X_{ik}}{X_i} = \log X_{ik} - \log X_{i}，其实这里也不严谨，\frac{a}{b}=\frac{c}{d}，但是得不出a=b的结果吧，\\这里就是因为左等式相等，右边分子分母就像等得到这个推理结果(玄学4)\\
$$
虽然上面已经四个玄学了，论文中还有一段论述，因为$w_i^T\hat w_k$在调换$i和k$的值后结果应该保持不变（两个向量内积不变），所以对等式右边也做一下调整（因为$\log X_i$不满足这个），所以对右边变化后，等式如下：
$$
w_i^T\hat w_k = \log X_{ik} - b_i - b_k，（玄学5）\\
即：\log X_{ik} = w_i^T\hat w_k + b_i + b_k，目标函数J为：\\
J = \sum_{i,j=1}^V w_i^T\hat w_k + b_i + b_k - \log X_{ik}，就是要用矩阵取拟合这个等式
$$
但是这里没有考虑权重，这样频率大的词还是会压制频率小的词权重都一样，下面的权重：
$$
J = \sum_{i,j=1}^V f(X_{ij}) w_i^T\hat w_k + b_i + b_k - \log X_{ik}
$$
